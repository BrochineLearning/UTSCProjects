{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the XGBoost classification algorithm on TESS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "df = pd.read_csv(\"TESSfield_19h_44d_combinedfeatures_try2.csv\", index_col=0)\n",
    "X = df.drop(['Ids', 'CatalogY', 'ManuleY', 'CombinedY', 'Catalog_Period',\n",
    "             'Depth', 'Catalog_Epoch', 'SNR'], axis=1)\n",
    "\n",
    "y = df['CombinedY']\n",
    "\n",
    "def modelfit(alg, X, y, cv_folds=4):\n",
    "    # StratifiedKFold automatically used by cross_val_predict on binary classification\n",
    "    # bear in mind that this does not use trapezfoid rule\n",
    "    # y_pred calculates the probabilities that each value is 1 or 0 using stratified cross validation\n",
    "    # pr_auc calculates the area under a precision recall curve\n",
    "    y_pred = cross_val_predict(alg, X, y, cv=cv_folds)\n",
    "    pr_auc = metrics.average_precision_score(y, y_pred)\n",
    "    return pr_auc\n",
    "\n",
    "xgb1 = XGBClassifier(objective='binary:logistic')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will get a baseline score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78547873498859899"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelfit(xgb1, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try adjusting depth, we will try 1, 6, 12, and 18. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing depth value 1\n",
      "0.767579061431\n",
      "testing depth value 6\n",
      "0.783890308475\n",
      "testing depth value 12\n",
      "0.80763570566\n",
      "testing depth value 18\n",
      "0.810930893584\n",
      "testing depth value 40\n",
      "0.807118399815\n"
     ]
    }
   ],
   "source": [
    "depth_vals = [1, 6, 12, 18, 40]\n",
    "\n",
    "for vals in depth_vals:\n",
    "    print 'testing depth value {0}'.format(vals)\n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=vals,    \n",
    "        objective='binary:logistic')\n",
    "    print modelfit(xgb, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try somewhere between 18 and 40. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " xgb = XGBClassifier(\n",
    "        max_depth=30,    \n",
    "        objective='binary:logistic')\n",
    "modelfit(xgb, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about one value up? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80445089166267381"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " xgb = XGBClassifier(\n",
    "        max_depth=19,    \n",
    "        objective='binary:logistic')\n",
    "modelfit(xgb, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we've hit the best score at 18. Next we'll do colsample_bytree, trying a range of values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing colsample_bytree value 0.2\n",
      "0.788727091189\n",
      "testing colsample_bytree value 0.4\n",
      "0.808734659418\n",
      "testing colsample_bytree value 0.6\n",
      "0.812794558806\n",
      "testing colsample_bytree value 0.8\n",
      "0.810824905179\n",
      "testing colsample_bytree value 1\n",
      "0.810930893584\n"
     ]
    }
   ],
   "source": [
    "colsample_vals = [.2, .4 ,.6,.8,1]\n",
    "\n",
    "for vals in colsample_vals:\n",
    "    print 'testing colsample_bytree value {0}'.format(vals)\n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=18,\n",
    "        colsample_bytree=vals,\n",
    "        objective='binary:logistic')\n",
    "    print modelfit(xgb, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".6 gets the best score, we'll now try .5 and .7 to be safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81172461896230519"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "        max_depth=18,\n",
    "        colsample_bytree=.5,\n",
    "        objective='binary:logistic')\n",
    "\n",
    "modelfit(xgb, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81181606461583311"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "        max_depth=18,\n",
    "        colsample_bytree=.7,\n",
    "        objective='binary:logistic')\n",
    "\n",
    "modelfit(xgb, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".812 is the best out of the three with a colsample_bytree value of .6\n",
    "\n",
    "Next is subsample, we'll try the same values as above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing subsample value 0.2\n",
      "0.791456280064\n",
      "testing subsample value 0.4\n",
      "0.800553718158\n",
      "testing subsample value 0.6\n",
      "0.806565170847\n",
      "testing subsample value 0.8\n",
      "0.812057861039\n",
      "testing subsample value 1\n",
      "0.810930893584\n"
     ]
    }
   ],
   "source": [
    "subsample_vals = [.2, .4, .6, .8, 1]\n",
    "\n",
    "for vals in sample_vals:\n",
    "    print 'testing subsample value {0}'.format(vals)\n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=18,\n",
    "        colsample_bytree=vals,\n",
    "        subsample=vals,\n",
    "        objective='binary:logistic')\n",
    "    print modelfit(xgb, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".8 seems to be the best, lets try .9 and .7 to be thorough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80972563161946054"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "        max_depth=18,\n",
    "        colsample_bytree=.6,\n",
    "        subsample=.9,\n",
    "        objective='binary:logistic')\n",
    "modelfit(xgb, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80206649134487595"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "        max_depth=18,\n",
    "        colsample_bytree=.6,\n",
    "        subsample=.7,\n",
    "        objective='binary:logistic')\n",
    "modelfit(xgb, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best score is ~.812 with a subsample val of .8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is min child weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing min_child value 0.1\n",
      "0.814750800284\n",
      "testing min_child value 0.5\n",
      "0.808962807971\n",
      "testing min_child value 1\n",
      "0.808414873856\n",
      "testing min_child value 3\n",
      "0.801848718167\n",
      "testing min_child value 5\n",
      "0.800883928252\n",
      "testing min_child value 7\n",
      "0.804091074989\n",
      "testing min_child value 10\n",
      "0.801430963606\n",
      "testing min_child value 175\n",
      "0.742936356452\n"
     ]
    }
   ],
   "source": [
    "min_child_weight = [.1, .5, 1, 3, 5, 7, 10, 175]\n",
    "\n",
    "for vals in min_child_weight:\n",
    "    print 'testing min_child value {0}'.format(vals)\n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=18,\n",
    "        colsample_bytree=.6,\n",
    "        subsample=.8,\n",
    "        min_child_weight=vals,\n",
    "        objective='binary:logistic')\n",
    "    print modelfit(xgb, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here .1 gives us the max score of ~.814"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing rates value 0.001, with n_estimators 100\n",
      "0.808393493593\n",
      "testing rates value 0.001, with n_estimators 500\n",
      "0.807281354588\n",
      "testing rates value 0.001, with n_estimators 1000\n",
      "0.809051303085\n",
      "testing rates value 0.001, with n_estimators 3000\n",
      "0.811904729499\n",
      "testing rates value 0.001, with n_estimators 5000\n",
      "0.81366790197\n",
      "testing rates value 0.001, with n_estimators 7000\n",
      "0.814347817636\n",
      "testing rates value 0.001, with n_estimators 9000\n",
      "0.814347817636\n",
      "testing rates value 0.001, with n_estimators 11000\n",
      "0.81366790197\n",
      "testing rates value 0.001, with n_estimators 13000\n",
      "0.814888406412\n",
      "testing rates value 0.01, with n_estimators 100\n",
      "0.81056038637\n",
      "testing rates value 0.01, with n_estimators 500\n",
      "0.817474871774\n",
      "testing rates value 0.01, with n_estimators 1000\n",
      "0.816937185069\n",
      "testing rates value 0.01, with n_estimators 3000\n",
      "0.817589610147\n",
      "testing rates value 0.01, with n_estimators 5000\n",
      "0.816242668622\n",
      "testing rates value 0.01, with n_estimators 7000\n",
      "0.816114671961\n",
      "testing rates value 0.01, with n_estimators 9000\n",
      "0.815446876474\n",
      "testing rates value 0.01, with n_estimators 11000\n",
      "0.815990238648\n",
      "testing rates value 0.01, with n_estimators 13000\n",
      "0.815446876474\n",
      "testing rates value 0.05, with n_estimators 100\n",
      "0.815700848222\n",
      "testing rates value 0.05, with n_estimators 500\n",
      "0.815158908289\n",
      "testing rates value 0.05, with n_estimators 1000\n",
      "0.815700848222\n",
      "testing rates value 0.05, with n_estimators 3000\n",
      "0.814662081516\n",
      "testing rates value 0.05, with n_estimators 5000\n",
      "0.812136155549\n",
      "testing rates value 0.05, with n_estimators 7000\n",
      "0.811923622291\n",
      "testing rates value 0.05, with n_estimators 9000\n",
      "0.811822443577\n",
      "testing rates value 0.05, with n_estimators 11000\n",
      "0.811822443577\n",
      "testing rates value 0.05, with n_estimators 13000\n",
      "0.811822443577\n",
      "testing rates value 0.07, with n_estimators 100\n",
      "0.815569361704\n",
      "testing rates value 0.07, with n_estimators 500\n",
      "0.811269548361\n",
      "testing rates value 0.07, with n_estimators 1000\n",
      "0.81158830077\n",
      "testing rates value 0.07, with n_estimators 3000\n",
      "0.81158830077\n",
      "testing rates value 0.07, with n_estimators 5000\n",
      "0.811923622291\n",
      "testing rates value 0.07, with n_estimators 7000\n",
      "0.811923622291\n",
      "testing rates value 0.07, with n_estimators 9000\n",
      "0.81117393008\n",
      "testing rates value 0.07, with n_estimators 11000\n",
      "0.811724618962\n",
      "testing rates value 0.07, with n_estimators 13000\n",
      "0.810527225546\n",
      "testing rates value 0.09, with n_estimators 100\n",
      "0.811500665141\n",
      "testing rates value 0.09, with n_estimators 500\n",
      "0.813147487822\n",
      "testing rates value 0.09, with n_estimators 1000\n",
      "0.812480829261\n",
      "testing rates value 0.09, with n_estimators 3000\n",
      "0.812908702172\n",
      "testing rates value 0.09, with n_estimators 5000\n",
      "0.81104031074\n",
      "testing rates value 0.09, with n_estimators 7000\n",
      "0.812136155549\n",
      "testing rates value 0.09, with n_estimators 9000\n",
      "0.812028183406\n",
      "testing rates value 0.09, with n_estimators 11000\n",
      "0.81247277364\n",
      "testing rates value 0.09, with n_estimators 13000\n",
      "0.81247277364\n",
      "testing rates value 0.1, with n_estimators 100\n",
      "0.814750800284\n",
      "testing rates value 0.1, with n_estimators 500\n",
      "0.815990238648\n",
      "testing rates value 0.1, with n_estimators 1000\n",
      "0.814662081516\n",
      "testing rates value 0.1, with n_estimators 3000\n",
      "0.813454824591\n",
      "testing rates value 0.1, with n_estimators 5000\n",
      "0.814662081516\n",
      "testing rates value 0.1, with n_estimators 7000\n",
      "0.813341417513\n",
      "testing rates value 0.1, with n_estimators 9000\n",
      "0.813231461633\n",
      "testing rates value 0.1, with n_estimators 11000\n",
      "0.812683875647\n",
      "testing rates value 0.1, with n_estimators 13000\n",
      "0.812683875647\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [1e-3, 1e-2, .05, .07, \n",
    "                .09, .1]\n",
    "n_estimators = [100, 500, 1000, 3000, 5000,\n",
    "               7000, 9000, 11000, 13000]\n",
    "\n",
    "for rates in learning_rate:\n",
    "    for estimators in n_estimators:\n",
    "        print 'testing rates value {0}, with n_estimators {1}'.format(rates, \n",
    "                                                                      estimators)\n",
    "        xgb = XGBClassifier(\n",
    "            max_depth=18,\n",
    "            colsample_bytree=.6,\n",
    "            subsample=.8,\n",
    "            min_child_weight=.1,\n",
    "            n_estimators=estimators, \n",
    "            learning_rate=rates,\n",
    "            objective='binary:logistic')\n",
    "            \n",
    "        print modelfit(xgb, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching the list gives a max value of 0.817589610147 using an eta(learning rate) of 0.01, with n_estimators 3000.\n",
    "\n",
    "Its also interesting to note that we get a very close score of 0.817474871774 with an eta of  0.01, and relatively few estimators of 500."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
