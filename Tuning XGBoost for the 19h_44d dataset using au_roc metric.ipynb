{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook the AUC_ROC error metric is used on the TESS 19h_44d simulate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "df = pd.read_csv(\"TESSfield_19h_44d_combinedfeatures_try2.csv\", index_col=0)\n",
    "X = df.drop(['Ids', 'CatalogY', 'ManuleY', 'CombinedY', 'Catalog_Period',\n",
    "             'Depth', 'Catalog_Epoch', 'SNR'], axis=1)\n",
    "\n",
    "y = df['CombinedY']\n",
    "\n",
    "def modelfit(alg, X, y, cv_folds=4):\n",
    "    # StratifiedKFold automatically used by cross_val_predict on binary classification\n",
    "    # bear in mind that this does not use trapezfoid rule\n",
    "    # y_pred calculates the probabilities that each value is 1 or 0 using stratified cross validation\n",
    "    # pr_auc calculates the area under a precision recall curve\n",
    "    y_pred = cross_val_predict(alg, X, y, cv=cv_folds, \n",
    "                              method='predict_proba')[:,1]\n",
    "    roc_auc = metrics.roc_auc_score(y, y_pred)\n",
    "    return roc_auc\n",
    "\n",
    "xgb1 = XGBClassifier(objective='binary:logistic')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First we get a baseline score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90860189955248649"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelfit(xgb1, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The outline dictates to start with max_depth, citing normal values as between 6 and 18. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing depth value 1\n",
      "0.909164086008\n",
      "testing depth value 6\n",
      "0.894389435916\n",
      "testing depth value 12\n",
      "0.900500625059\n",
      "testing depth value 18\n",
      "0.901362294336\n",
      "testing depth value 40\n",
      "0.899001036477\n"
     ]
    }
   ],
   "source": [
    "depth_vals = [1, 6, 12, 18, 40]\n",
    "\n",
    "for vals in depth_vals:\n",
    "    print 'testing depth value {0}'.format(vals)\n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=vals,    \n",
    "        objective='binary:logistic')\n",
    "    print modelfit(xgb, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest value is ~.909 at max_depth = 1. \n",
    "\n",
    "Next is colsample_bytree with standard values between 0.5-1 but we'll add periphery values for the sake of curiosity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing colsample_bytree value 0.2\n",
      "0.911654283377\n",
      "testing colsample_bytree value 0.4\n",
      "0.90861720581\n",
      "testing colsample_bytree value 0.6\n",
      "0.909458591323\n",
      "testing colsample_bytree value 0.8\n",
      "0.909112782013\n",
      "testing colsample_bytree value 1\n",
      "0.909164086008\n"
     ]
    }
   ],
   "source": [
    "colsample_vals = [.2, .4 , .6, .8, 1]\n",
    "\n",
    "for vals in colsample_vals:\n",
    "    print 'testing colsample_bytree value {0}'.format(vals)\n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=1,\n",
    "        colsample_bytree=vals,\n",
    "        objective='binary:logistic')\n",
    "    print modelfit(xgb, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colsample_bytree value of .2 has the best score, bringing the score up to ~.9116 \n",
    "\n",
    "Next to tune is subsample, with normal values in the range 0.5-1 like above, we'll add periphery values as well for the sake of curiosity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing subsample value 0.2\n",
      "0.91161908408\n",
      "testing subsample value 0.4\n",
      "0.910092297585\n",
      "testing subsample value 0.6\n",
      "0.910194888586\n",
      "testing subsample value 0.8\n",
      "0.910826012661\n",
      "testing subsample value 1\n",
      "0.911654283377\n"
     ]
    }
   ],
   "source": [
    "sample_vals = [.2, .4, .6, .8, 1]\n",
    "\n",
    "for vals in sample_vals:\n",
    "    print 'testing subsample value {0}'.format(vals)\n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=1,\n",
    "        colsample_bytree=.2,\n",
    "        subsample=vals,\n",
    "        objective='binary:logistic')\n",
    "    print modelfit(xgb, X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "We see the 1 has the highest, like with colsample_bytree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing min_child value 0.1\n",
      "0.91169119847\n",
      "testing min_child value 0.5\n",
      "0.91169119847\n",
      "testing min_child value 1\n",
      "0.911654283377\n",
      "testing min_child value 3\n",
      "0.911609485816\n",
      "testing min_child value 5\n",
      "0.911915814833\n",
      "testing min_child value 7\n",
      "0.911915814833\n",
      "testing min_child value 10\n",
      "0.911929931926\n",
      "testing min_child value 175\n",
      "0.915413201354\n"
     ]
    }
   ],
   "source": [
    "min_child_weight = [.1, .5, 1, 3, 5, 7, 10, 175]\n",
    "\n",
    "for vals in min_child_weight:\n",
    "    print 'testing min_child value {0}'.format(vals)\n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=1,\n",
    "        colsample_bytree=.2,\n",
    "        subsample=1,\n",
    "        min_child_weight=vals,\n",
    "        objective='binary:logistic')\n",
    "    print modelfit(xgb, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max value is 0.915413201354 at a high min_child of 175, let's see if going higher increases the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.915720192906\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "        max_depth=1,\n",
    "        colsample_bytree=.2,\n",
    "        subsample=1,\n",
    "        min_child_weight=170,\n",
    "        objective='binary:logistic')\n",
    "\n",
    "print modelfit(xgb, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decreasing it by 5 also caused an increase, lets just try scores in that range, since the algorithm gives results quickly we'll try a broad range.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score was :0.91628353455, for min_child: 158\n"
     ]
    }
   ],
   "source": [
    "min_child_weight = range(150, 200)\n",
    "min_child = list()\n",
    "scores=list()\n",
    "\n",
    "for vals in min_child_weight:\n",
    "#     print 'testing min_child value {0}'.format(vals)\n",
    "    min_child.append(vals)\n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=1,\n",
    "        colsample_bytree=.2,\n",
    "        subsample=1,\n",
    "        min_child_weight=vals,\n",
    "        objective='binary:logistic')\n",
    "    score = modelfit(xgb, X, y)\n",
    "    scores.append(score)\n",
    "\n",
    "print \"The best score was :{0}, for min_child: {1}\".format(max(scores), min_child[scores.index(max(scores))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing rates value 0.001, with n_estimators 100\n",
      "0.885354904387\n",
      "testing rates value 0.001, with n_estimators 500\n",
      "0.886195355555\n",
      "testing rates value 0.001, with n_estimators 1000\n",
      "0.888254871178\n",
      "testing rates value 0.001, with n_estimators 3000\n",
      "0.89012493576\n",
      "testing rates value 0.001, with n_estimators 5000\n",
      "0.904767231738\n",
      "testing rates value 0.001, with n_estimators 7000\n",
      "0.912527028881\n",
      "testing rates value 0.001, with n_estimators 9000\n",
      "0.915730827442\n",
      "testing rates value 0.001, with n_estimators 11000\n",
      "0.916850086942\n",
      "testing rates value 0.001, with n_estimators 13000\n",
      "0.91719431636\n",
      "testing rates value 0.01, with n_estimators 100\n",
      "0.888356969525\n",
      "testing rates value 0.01, with n_estimators 500\n",
      "0.903279976499\n",
      "testing rates value 0.01, with n_estimators 1000\n",
      "0.916295426205\n",
      "testing rates value 0.01, with n_estimators 3000\n",
      "0.917440677463\n",
      "testing rates value 0.01, with n_estimators 5000\n",
      "0.916808788925\n",
      "testing rates value 0.01, with n_estimators 7000\n",
      "0.91644261091\n",
      "testing rates value 0.01, with n_estimators 9000\n",
      "0.916207597843\n",
      "testing rates value 0.01, with n_estimators 11000\n",
      "0.916097362207\n",
      "testing rates value 0.01, with n_estimators 13000\n",
      "0.916008667453\n",
      "testing rates value 0.05, with n_estimators 100\n",
      "0.90333308123\n",
      "testing rates value 0.05, with n_estimators 500\n",
      "0.918478818891\n",
      "testing rates value 0.05, with n_estimators 1000\n",
      "0.917863272884\n",
      "testing rates value 0.05, with n_estimators 3000\n",
      "0.917139648726\n",
      "testing rates value 0.05, with n_estimators 5000\n",
      "0.916857646636\n",
      "testing rates value 0.05, with n_estimators 7000\n",
      "0.916705212615\n",
      "testing rates value 0.05, with n_estimators 9000\n",
      "0.916570089445\n",
      "testing rates value 0.05, with n_estimators 11000\n",
      "0.916524561397\n",
      "testing rates value 0.05, with n_estimators 13000\n",
      "0.916432519992\n",
      "testing rates value 0.07, with n_estimators 100\n",
      "0.914290017608\n",
      "testing rates value 0.07, with n_estimators 500\n",
      "0.918584094008\n",
      "testing rates value 0.07, with n_estimators 1000\n",
      "0.918219376968\n",
      "testing rates value 0.07, with n_estimators 3000\n",
      "0.917919928123\n",
      "testing rates value 0.07, with n_estimators 5000\n",
      "0.91770502894\n",
      "testing rates value 0.07, with n_estimators 7000\n",
      "0.917508408929\n",
      "testing rates value 0.07, with n_estimators 9000\n",
      "0.917331410147\n",
      "testing rates value 0.07, with n_estimators 11000\n",
      "0.917179519744\n",
      "testing rates value 0.07, with n_estimators 13000\n",
      "0.91708102287\n",
      "testing rates value 0.09, with n_estimators 100\n",
      "0.915971276694\n",
      "testing rates value 0.09, with n_estimators 500\n",
      "0.918095364001\n",
      "testing rates value 0.09, with n_estimators 1000\n",
      "0.917978401086\n",
      "testing rates value 0.09, with n_estimators 3000\n",
      "0.917612274036\n",
      "testing rates value 0.09, with n_estimators 5000\n",
      "0.91731335182\n",
      "testing rates value 0.09, with n_estimators 7000\n",
      "0.917058700536\n",
      "testing rates value 0.09, with n_estimators 9000\n",
      "0.916828970761\n",
      "testing rates value 0.09, with n_estimators 11000\n",
      "0.916654078501\n",
      "testing rates value 0.09, with n_estimators 13000\n",
      "0.916587298368\n",
      "testing rates value 0.1, with n_estimators 100\n",
      "0.91628353455\n",
      "testing rates value 0.1, with n_estimators 500\n",
      "0.917463050761\n",
      "testing rates value 0.1, with n_estimators 1000\n",
      "0.917424929515\n",
      "testing rates value 0.1, with n_estimators 3000\n",
      "0.91693140887\n",
      "testing rates value 0.1, with n_estimators 5000\n",
      "0.91655987961\n",
      "testing rates value 0.1, with n_estimators 7000\n",
      "0.91636864482\n",
      "testing rates value 0.1, with n_estimators 9000\n",
      "0.916204098299\n",
      "testing rates value 0.1, with n_estimators 11000\n",
      "0.916037343328\n",
      "testing rates value 0.1, with n_estimators 13000\n",
      "0.915931235795\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [1e-3, 1e-2, .05, .07, \n",
    "                .09, .1]\n",
    "n_estimators = [100, 500, 1000, 3000, 5000,\n",
    "               7000, 9000, 11000, 13000]\n",
    "\n",
    "for rates in learning_rate:\n",
    "    for estimators in n_estimators:\n",
    "        print 'testing rates value {0}, with n_estimators {1}'.format(rates, \n",
    "                                                                    estimators)\n",
    "\n",
    "        xgb = XGBClassifier(\n",
    "        max_depth=1,\n",
    "        objective='binary:logistic',\n",
    "        colsample_bytree=.2,\n",
    "        subsample=1,\n",
    "        min_child_weight=158,\n",
    "        n_estimators=estimators, \n",
    "        learning_rate=rates)\n",
    "        \n",
    "        print modelfit(xgb, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So the final eta value here is 0.07 with n_estimators 500\n",
    "and a score of 0.918584094008. \n",
    "\n",
    "\n",
    "Now we'll do the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The roc_auc score is: 0.905512732443\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"TESSfield_12h_-20d_combinedfeatures_try2.csv\", index_col=0)\n",
    "X_test = df_test.drop(['Ids', 'CatalogY', 'ManuleY', 'CombinedY', 'Catalog_Period',\n",
    "             'Depth', 'Catalog_Epoch', 'SNR'], axis=1)\n",
    "\n",
    "y_test = df_test['CombinedY']\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "        max_depth=1,\n",
    "        objective='binary:logistic',\n",
    "        colsample_bytree=.2,\n",
    "        subsample=1,\n",
    "        min_child_weight=158,\n",
    "        n_estimators=500, \n",
    "        learning_rate=.07)\n",
    "\n",
    "xgb.fit(X, y) # fitting on the 19h set\n",
    "\n",
    "y_pred_test = xgb.predict_proba(X_test)[:, 1] # testing on the 12h set \n",
    "\n",
    "pr_auc = metrics.average_precision_score(y_test, y_pred_test)\n",
    "roc_auc = metrics.roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print 'The roc_auc score is: {0}'.format(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Finally we'll try ensembling the pr_auc and roc_auc fitted models to see the effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pr_roc: 0.907411232318\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "estimators = []\n",
    "\n",
    "xgb_pr_auc = XGBClassifier(\n",
    "          max_depth=1,\n",
    "          colsample_bytree=.4,\n",
    "          subsample= .4,\n",
    "          min_child_weight=7,\n",
    "          n_estimators=3000, \n",
    "          learning_rate=.01,\n",
    "          objective='binary:logistic')\n",
    "\n",
    "estimators.append(('xgb1', xgb_pr_auc))\n",
    "\n",
    "xgb_roc_auc = XGBClassifier(\n",
    "        max_depth=1,\n",
    "        objective='binary:logistic',\n",
    "        colsample_bytree=.2,\n",
    "        subsample=1,\n",
    "        min_child_weight=158,\n",
    "        n_estimators=500, \n",
    "        learning_rate=.07)\n",
    "\n",
    "\n",
    "estimators.append(('xgb2', xgb_roc_auc))\n",
    "\n",
    "# set voting = soft to get probabilities \n",
    "ensemble = VotingClassifier(estimators, n_jobs=-1, voting='soft')\n",
    "\n",
    "\n",
    "ensemble.fit(X, y)\n",
    "y_pred = ensemble.predict_proba(X_test)[:, 1]\n",
    "\n",
    "pr_auc = metrics.average_precision_score(y_test, y_pred)\n",
    "pr_roc = metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print 'pr_roc: {0}'.format(pr_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this simple ensemble caused a minor increase in score at the 3rd decimal place. However future aditions of algorithms should include models that aren't GBM in order to increase the diversity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
