{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The following notebook uses combined BLS and PCA features generated from simulated TESS data and tests the effects of dropping features on the default score of the XGBoost machine learning algorithm    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# open Chelsea's combined feature file\n",
    "# remove the last row to make the dimensions match with the raw LC file\n",
    "data_combined_features = pd.read_csv(\"TESSfield_05h_01d_combinedfeatures.csv\",\n",
    "                                     header=0, index_col=0)\n",
    "data_combined_features = data_combined_features.drop(data_combined_features.index[-1])\n",
    "\n",
    "# drop the columns that aren't features and get targets \n",
    "X = data_combined_features.drop(['Ids', 'CatalogY', 'ManuleY', 'CombinedY',\n",
    "                                 'Catalog_Period', 'Depth', 'Catalog_Epoch', 'SNR'],\n",
    "                                axis=1)\n",
    "\n",
    "y = data_combined_features['CombinedY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods we'll try are the \"lazy\" ones, we'll just delete a feature off the tail/the head one at a time regardless of its effect on the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing features linearly\n",
      "0.699410990974\n",
      "dropping P19\n",
      "0.70257779817\n",
      "dropping P18\n",
      "0.701695051255\n",
      "dropping P17\n",
      "0.708411308736\n",
      "dropping P16\n",
      "0.705255657316\n",
      "dropping P15\n",
      "0.703905570515\n",
      "dropping P14\n",
      "0.699666604825\n",
      "dropping P13\n",
      "0.697122253662\n",
      "dropping P12\n",
      "0.693576606645\n",
      "dropping P11\n",
      "0.702092662611\n",
      "dropping P10\n",
      "0.714701020128\n",
      "dropping P9\n",
      "0.706627389138\n",
      "dropping P8\n",
      "0.695744191453\n",
      "dropping P7\n",
      "0.696236533057\n",
      "dropping P6\n",
      "0.691200028507\n",
      "dropping P5\n",
      "0.7007406319\n",
      "dropping P4\n",
      "0.693643577149\n",
      "dropping P3\n",
      "0.698521507342\n",
      "dropping P2\n",
      "0.710733512794\n",
      "dropping P1\n",
      "0.698922212224\n",
      "dropping P0\n",
      "0.698922212224\n",
      "dropping BLS_SignaltoPinknoise_1_0\n",
      "0.672466013053\n",
      "dropping BLS_Whitenoise_1_0\n",
      "0.67003291743\n",
      "dropping BLS_Rednoise_1_0\n",
      "0.675139052105\n",
      "dropping BLS_Npointsaftertransit_1_0\n",
      "0.665400745733\n",
      "dropping BLS_Npointsbeforetransit_1_0\n",
      "0.665400745733\n",
      "dropping BLS_Ntransits_1_0\n",
      "0.665400745733\n",
      "dropping BLS_Npointsintransit_1_0\n",
      "0.680849933295\n",
      "dropping BLS_fraconenight_1_0\n",
      "0.67003291743\n",
      "dropping BLS_deltaChi2_1_0\n",
      "0.648708136243\n",
      "dropping BLS_i2_1_0\n",
      "0.649832461222\n",
      "dropping BLS_i1_1_0\n",
      "0.647942236747\n",
      "dropping BLS_OOTmag_1_0\n",
      "0.64062143736\n",
      "dropping BLS_Qingress_1_0\n",
      "0.649832461222\n",
      "dropping BLS_Qtran_1_0\n",
      "0.61049161336\n",
      "dropping BLS_Depth_1_0\n",
      "0.602062670755\n",
      "dropping BLS_SDE_1_0\n",
      "0.588689268218\n",
      "dropping BLS_SR_1_0\n",
      "0.581993327917\n",
      "dropping BLS_SN_1_0\n",
      "0.258667747638\n",
      "dropping BLS_Tc_1_0\n",
      "0.506235409755\n",
      "dropping BLS_Period_1_0\n",
      "the max score was 0.714701020128\n"
     ]
    }
   ],
   "source": [
    "xgb1 = XGBClassifier(objective='binary:logistic')\n",
    "\n",
    "\n",
    "\n",
    "def modelfit(alg, X, y, cv_folds=4):\n",
    "    \n",
    "    # StratifiedKFold automatically used by cross_val_predict on binary classification\n",
    "    # bear in mind that this does not use trapezfoid rule\n",
    "    # y_pred calculates the probabilities that each value is 1 or 0 using stratified cross validation\n",
    "    # pr_auc calculates the area under a precision recall curve\n",
    "    y_pred = cross_val_predict(alg, X, y, cv=cv_folds)\n",
    "    pr_auc = metrics.average_precision_score(y, y_pred)\n",
    "    return pr_auc\n",
    "\n",
    "def feature_testing(alg, X, y):\n",
    "    score_list = []\n",
    "    print 'testing features linearly'\n",
    "    X = X\n",
    "    while len(X.columns) > 0:\n",
    "        score = modelfit(alg, X, y)\n",
    "        print score\n",
    "        score_list.append(score)\n",
    "        print 'dropping {0}'.format(X.columns[-1])\n",
    "        X.drop(X.columns[-1], axis=1, inplace=True)   \n",
    "    print 'the max score was {0}'.format(max(score_list))\n",
    "    \n",
    "\n",
    "feature_testing(xgb1, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad... we see that we can improve the score significantly just by arbitrarily deleting. What if we go in the other direction? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing features linearly\n",
      "0.699410990974\n",
      "dropping BLS_Period_1_0\n",
      "0.7007406319\n",
      "dropping BLS_Tc_1_0\n",
      "0.708888928806\n",
      "dropping BLS_SN_1_0\n",
      "0.690747751006\n",
      "dropping BLS_SR_1_0\n",
      "0.677529510817\n",
      "dropping BLS_SDE_1_0\n",
      "0.69309749028\n",
      "dropping BLS_Depth_1_0\n",
      "0.703466408039\n",
      "dropping BLS_Qtran_1_0\n",
      "0.685181619696\n",
      "dropping BLS_Qingress_1_0\n",
      "0.686898037606\n",
      "dropping BLS_OOTmag_1_0\n",
      "0.688631705948\n",
      "dropping BLS_i1_1_0\n",
      "0.686568538615\n",
      "dropping BLS_i2_1_0\n",
      "0.684800265022\n",
      "dropping BLS_deltaChi2_1_0\n",
      "0.644127599675\n",
      "dropping BLS_fraconenight_1_0\n",
      "0.66559493405\n",
      "dropping BLS_Npointsintransit_1_0\n",
      "0.660582119336\n",
      "dropping BLS_Ntransits_1_0\n",
      "0.665502019408\n",
      "dropping BLS_Npointsbeforetransit_1_0\n",
      "0.66576554877\n",
      "dropping BLS_Npointsaftertransit_1_0\n",
      "0.484877534492\n",
      "dropping BLS_Rednoise_1_0\n",
      "0.452175682734\n",
      "dropping BLS_Whitenoise_1_0\n",
      "0.426157160814\n",
      "dropping BLS_SignaltoPinknoise_1_0\n",
      "0.174016767494\n",
      "dropping P0\n",
      "0.138532423404\n",
      "dropping P1\n",
      "0.158821904145\n",
      "dropping P2\n",
      "0.25659510827\n",
      "dropping P3\n",
      "0.196968527742\n",
      "dropping P4\n",
      "0.217706219381\n",
      "dropping P5\n",
      "0.229144127878\n",
      "dropping P6\n",
      "0.157285988059\n",
      "dropping P7\n",
      "0.189928441603\n",
      "dropping P8\n",
      "0.199607156298\n",
      "dropping P9\n",
      "0.197300025289\n",
      "dropping P10\n",
      "0.220829437053\n",
      "dropping P11\n",
      "0.207472301252\n",
      "dropping P12\n",
      "0.14339709917\n",
      "dropping P13\n",
      "0.128917038789\n",
      "dropping P14\n",
      "0.121227919182\n",
      "dropping P15\n",
      "0.121227919182\n",
      "dropping P16\n",
      "0.136100085521\n",
      "dropping P17\n",
      "0.261100085521\n",
      "dropping P18\n",
      "0.0062354097555\n",
      "dropping P19\n",
      "the max score was 0.708888928806\n"
     ]
    }
   ],
   "source": [
    "# drop the columns that aren't features and get targets \n",
    "X = data_combined_features.drop(['Ids', 'CatalogY', 'ManuleY', 'CombinedY',\n",
    "                                 'Catalog_Period', 'Depth', 'Catalog_Epoch', 'SNR'],\n",
    "                                axis=1)\n",
    "\n",
    "xgb1 = XGBClassifier(objective='binary:logistic')\n",
    "\n",
    "\n",
    "\n",
    "def modelfit(alg, X, y, cv_folds=4):\n",
    "    \n",
    "    # StratifiedKFold automatically used by cross_val_predict on binary classification\n",
    "    # bear in mind that this does not use trapezfoid rule\n",
    "    # y_pred calculates the probabilities that each value is 1 or 0 using stratified cross validation\n",
    "    # pr_auc calculates the area under a precision recall curve\n",
    "    y_pred = cross_val_predict(alg, X, y, cv=cv_folds)\n",
    "    pr_auc = metrics.average_precision_score(y, y_pred)\n",
    "    return pr_auc\n",
    "\n",
    "def feature_testing(alg, X, y):\n",
    "    score_list = []\n",
    "    print 'testing features linearly'\n",
    "    X = X\n",
    "    while len(X.columns) > 0:\n",
    "        score = modelfit(alg, X, y)\n",
    "        print score\n",
    "        score_list.append(score)\n",
    "        print 'dropping {0}'.format(X.columns[0])\n",
    "        X.drop(X.columns[0], axis=1, inplace=True)   \n",
    "    print 'the max score was {0}'.format(max(score_list))\n",
    "    \n",
    "\n",
    "feature_testing(xgb1, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is intuitive, the BLS features are much more important that the PCA features, even so we still manage to increase the score noticeably. \n",
    "\n",
    "Next, we'll only remove a feature if it causes a positive increase in score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing features linearly\n",
      "The score with feature BLS_Period_1_0 is: 0.699410990974, the score without that feature is 0.7007406319\n",
      "The score is higher without feature BLS_Period_1_0 so it has been dropped\n",
      "The score with feature BLS_Tc_1_0 is: 0.7007406319, the score without that feature is 0.708888928806\n",
      "The score is higher without feature BLS_Tc_1_0 so it has been dropped\n",
      "The score with feature BLS_SN_1_0 is: 0.708888928806, the score without that feature is 0.690747751006\n",
      "The score with feature BLS_SR_1_0 is: 0.708888928806, the score without that feature is 0.6917438811\n",
      "The score with feature BLS_SDE_1_0 is: 0.708888928806, the score without that feature is 0.7007406319\n",
      "The score with feature BLS_Depth_1_0 is: 0.708888928806, the score without that feature is 0.701899562157\n",
      "The score with feature BLS_Qtran_1_0 is: 0.708888928806, the score without that feature is 0.692558483229\n",
      "The score with feature BLS_Qingress_1_0 is: 0.708888928806, the score without that feature is 0.70758745564\n",
      "The score with feature BLS_OOTmag_1_0 is: 0.708888928806, the score without that feature is 0.708888928806\n",
      "The score with feature BLS_i1_1_0 is: 0.708888928806, the score without that feature is 0.7007406319\n",
      "The score with feature BLS_i2_1_0 is: 0.708888928806, the score without that feature is 0.696821721629\n",
      "The score with feature BLS_deltaChi2_1_0 is: 0.708888928806, the score without that feature is 0.67702221006\n",
      "The score with feature BLS_fraconenight_1_0 is: 0.708888928806, the score without that feature is 0.711346046706\n",
      "The score is higher without feature BLS_fraconenight_1_0 so it has been dropped\n",
      "The score with feature BLS_Npointsintransit_1_0 is: 0.711346046706, the score without that feature is 0.719103138904\n",
      "The score is higher without feature BLS_Npointsintransit_1_0 so it has been dropped\n",
      "The score with feature BLS_Ntransits_1_0 is: 0.719103138904, the score without that feature is 0.710733512794\n",
      "The score with feature BLS_Npointsbeforetransit_1_0 is: 0.719103138904, the score without that feature is 0.701273037606\n",
      "The score with feature BLS_Npointsaftertransit_1_0 is: 0.719103138904, the score without that feature is 0.714701020128\n",
      "The score with feature BLS_Rednoise_1_0 is: 0.719103138904, the score without that feature is 0.688955495642\n",
      "The score with feature BLS_Whitenoise_1_0 is: 0.719103138904, the score without that feature is 0.716382551255\n",
      "The score with feature BLS_SignaltoPinknoise_1_0 is: 0.719103138904, the score without that feature is 0.672466013053\n",
      "The score with feature P0 is: 0.719103138904, the score without that feature is 0.721419055805\n",
      "The score is higher without feature P0 so it has been dropped\n",
      "The score with feature P1 is: 0.721419055805, the score without that feature is 0.715183766004\n",
      "The score with feature P2 is: 0.721419055805, the score without that feature is 0.714480025349\n",
      "The score with feature P3 is: 0.721419055805, the score without that feature is 0.697122253662\n",
      "The score with feature P4 is: 0.721419055805, the score without that feature is 0.712597113111\n",
      "The score with feature P5 is: 0.721419055805, the score without that feature is 0.703905570515\n",
      "The score with feature P6 is: 0.721419055805, the score without that feature is 0.706977402686\n",
      "The score with feature P7 is: 0.721419055805, the score without that feature is 0.712033480509\n",
      "The score with feature P8 is: 0.721419055805, the score without that feature is 0.699992015513\n",
      "The score with feature P9 is: 0.721419055805, the score without that feature is 0.70758745564\n",
      "The score with feature P10 is: 0.721419055805, the score without that feature is 0.701273037606\n",
      "The score with feature P11 is: 0.721419055805, the score without that feature is 0.703154582109\n",
      "The score with feature P12 is: 0.721419055805, the score without that feature is 0.710119849174\n",
      "The score with feature P13 is: 0.721419055805, the score without that feature is 0.695563590802\n",
      "The score with feature P14 is: 0.721419055805, the score without that feature is 0.704434002432\n",
      "The score with feature P15 is: 0.721419055805, the score without that feature is 0.713254859969\n",
      "The score with feature P16 is: 0.721419055805, the score without that feature is 0.712055243214\n",
      "The score with feature P17 is: 0.721419055805, the score without that feature is 0.706977402686\n",
      "The score with feature P18 is: 0.721419055805, the score without that feature is 0.713782639614\n",
      "The score with feature P19 is: 0.721419055805, the score without that feature is 0.716382551255\n",
      "the dropped features are: ['BLS_Period_1_0', 'BLS_Tc_1_0', 'BLS_fraconenight_1_0', 'BLS_Npointsintransit_1_0', 'P0']\n"
     ]
    }
   ],
   "source": [
    "# the model fitting function \n",
    "\n",
    "# drop the columns that aren't features and get targets \n",
    "X = data_combined_features.drop(['Ids', 'CatalogY', 'ManuleY', 'CombinedY',\n",
    "                                 'Catalog_Period', 'Depth', 'Catalog_Epoch', 'SNR'],\n",
    "                                axis=1)\n",
    "\n",
    "y = data_combined_features['CombinedY']\n",
    "\n",
    "\n",
    "def feature_testing(alg, X, y):\n",
    "    score_perm = modelfit(alg, X, y)  # test the model without the feature dropped\n",
    "    drop_list = []\n",
    "    print 'testing features linearly'\n",
    "    X = X\n",
    "    X_copy = X.copy()\n",
    "    for column in X.columns:\n",
    "        df_temp = X.drop(column, axis=1)  # temporarily drop features\n",
    "        score_temp = modelfit(alg, df_temp, y)  # test the model with the feature dropped\n",
    "        print 'The score with feature {0} is: {1}, the score without that feature is {2}'.format(column, score_perm,\n",
    "                                                                                                 score_temp)\n",
    "        \n",
    "        if score_temp > score_perm: \n",
    "            X.drop(column, axis=1, inplace=True)\n",
    "            score_perm = modelfit(alg, X, y)\n",
    "            drop_list.append(column)\n",
    "            \n",
    "            print 'The score is higher without feature {0} so it has been dropped'.format(column)\n",
    "    print 'the dropped features are: {0}'.format(drop_list)\n",
    "\n",
    "feature_testing(xgb1, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by conditioning on a score increase causes the best score yet of 0.721419055805. \n",
    "\n",
    "We'll follow suite and try the same but in reverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing features linearly from right to left\n",
      "The score with feature P19 is: 0.699410990974, the score without that feature is 0.70257779817\n",
      "The score is higher without feature P19 so it has been dropped\n",
      "The score with feature P18 is: 0.70257779817, the score without that feature is 0.701695051255\n",
      "The score with feature P17 is: 0.70257779817, the score without that feature is 0.697122253662\n",
      "The score with feature P16 is: 0.70257779817, the score without that feature is 0.7007406319\n",
      "The score with feature P15 is: 0.70257779817, the score without that feature is 0.692558483229\n",
      "The score with feature P14 is: 0.70257779817, the score without that feature is 0.703905570515\n",
      "The score is higher without feature P14 so it has been dropped\n",
      "The score with feature P13 is: 0.703905570515, the score without that feature is 0.688955495642\n",
      "The score with feature P12 is: 0.703905570515, the score without that feature is 0.7007406319\n",
      "The score with feature P11 is: 0.703905570515, the score without that feature is 0.698521507342\n",
      "The score with feature P10 is: 0.703905570515, the score without that feature is 0.683686805251\n",
      "The score with feature P9 is: 0.703905570515, the score without that feature is 0.70257779817\n",
      "The score with feature P8 is: 0.703905570515, the score without that feature is 0.702092662611\n",
      "The score with feature P7 is: 0.703905570515, the score without that feature is 0.708888928806\n",
      "The score is higher without feature P7 so it has been dropped\n",
      "The score with feature P6 is: 0.708888928806, the score without that feature is 0.708888928806\n",
      "The score with feature P5 is: 0.708888928806, the score without that feature is 0.70706307156\n",
      "The score with feature P4 is: 0.708888928806, the score without that feature is 0.705255657316\n",
      "The score with feature P3 is: 0.708888928806, the score without that feature is 0.7007406319\n",
      "The score with feature P2 is: 0.708888928806, the score without that feature is 0.698922212224\n",
      "The score with feature P1 is: 0.708888928806, the score without that feature is 0.70257779817\n",
      "The score with feature P0 is: 0.708888928806, the score without that feature is 0.70706307156\n",
      "The score with feature BLS_SignaltoPinknoise_1_0 is: 0.708888928806, the score without that feature is 0.677999463751\n",
      "The score with feature BLS_Whitenoise_1_0 is: 0.708888928806, the score without that feature is 0.705255657316\n",
      "The score with feature BLS_Rednoise_1_0 is: 0.708888928806, the score without that feature is 0.705255657316\n",
      "The score with feature BLS_Npointsaftertransit_1_0 is: 0.708888928806, the score without that feature is 0.694387979516\n",
      "The score with feature BLS_Npointsbeforetransit_1_0 is: 0.708888928806, the score without that feature is 0.710213247724\n",
      "The score is higher without feature BLS_Npointsbeforetransit_1_0 so it has been dropped\n",
      "The score with feature BLS_Ntransits_1_0 is: 0.710213247724, the score without that feature is 0.710213247724\n",
      "The score with feature BLS_Npointsintransit_1_0 is: 0.710213247724, the score without that feature is 0.702092662611\n",
      "The score with feature BLS_fraconenight_1_0 is: 0.710213247724, the score without that feature is 0.698922212224\n",
      "The score with feature BLS_deltaChi2_1_0 is: 0.710213247724, the score without that feature is 0.684060767065\n",
      "The score with feature BLS_i2_1_0 is: 0.710213247724, the score without that feature is 0.698104443179\n",
      "The score with feature BLS_i1_1_0 is: 0.710213247724, the score without that feature is 0.700298066908\n",
      "The score with feature BLS_OOTmag_1_0 is: 0.710213247724, the score without that feature is 0.710213247724\n",
      "The score with feature BLS_Qingress_1_0 is: 0.710213247724, the score without that feature is 0.704434002432\n",
      "The score with feature BLS_Qtran_1_0 is: 0.710213247724, the score without that feature is 0.685751310558\n",
      "The score with feature BLS_Depth_1_0 is: 0.710213247724, the score without that feature is 0.703905570515\n",
      "The score with feature BLS_SDE_1_0 is: 0.710213247724, the score without that feature is 0.717606770582\n",
      "The score is higher without feature BLS_SDE_1_0 so it has been dropped\n",
      "The score with feature BLS_SR_1_0 is: 0.717606770582, the score without that feature is 0.712927708469\n",
      "The score with feature BLS_SN_1_0 is: 0.717606770582, the score without that feature is 0.693576606645\n",
      "The score with feature BLS_Tc_1_0 is: 0.717606770582, the score without that feature is 0.70758745564\n",
      "The score with feature BLS_Period_1_0 is: 0.717606770582, the score without that feature is 0.708204721039\n",
      "the dropped features are: ['P19', 'P14', 'P7', 'BLS_Npointsbeforetransit_1_0', 'BLS_SDE_1_0']\n"
     ]
    }
   ],
   "source": [
    "X = data_combined_features.drop(['Ids', 'CatalogY', 'ManuleY', 'CombinedY',\n",
    "                                 'Catalog_Period', 'Depth', 'Catalog_Epoch', 'SNR'],\n",
    "                                axis=1)\n",
    "\n",
    "def feature_testing(alg, X, y):\n",
    "    score_perm = modelfit(alg, X, y)  # test the model without the feature dropped\n",
    "    drop_list = []\n",
    "    print 'testing features linearly from right to left'\n",
    "    X = X\n",
    "    X_copy = X.copy()\n",
    "    for column in reversed(X.columns):\n",
    "        df_temp = X.drop(column, axis=1)  # temporarily drop features\n",
    "        score_temp = modelfit(alg, df_temp, y)  # test the model with the feature dropped\n",
    "        print 'The score with feature {0} is: {1}, the score without that feature is {2}'.format(column, score_perm,\n",
    "                                                                                                 score_temp)\n",
    "        \n",
    "        if score_temp > score_perm: \n",
    "            X.drop(column, axis=1, inplace=True)\n",
    "            score_perm = modelfit(alg, X, y)\n",
    "            drop_list.append(column)\n",
    "            \n",
    "            print 'The score is higher without feature {0} so it has been dropped'.format(column)\n",
    "    print 'the dropped features are: {0}'.format(drop_list)\n",
    "\n",
    "feature_testing(xgb1, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as good as last time, but an increase nonetheless. \n",
    "\n",
    "Lets try introducing a threshold, to see if cutting out only features that cause a certain change in threshold will yield better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing features linearly based on threshold\n",
      "The score with feature BLS_Period_1_0 is: 0.699410990974, the score without that feature is 0.7007406319\n",
      "The score with feature BLS_Tc_1_0 is: 0.699410990974, the score without that feature is 0.696236533057\n",
      "The score with feature BLS_SN_1_0 is: 0.699410990974, the score without that feature is 0.689364968058\n",
      "The score with feature BLS_SR_1_0 is: 0.699410990974, the score without that feature is 0.699410990974\n",
      "The score with feature BLS_SDE_1_0 is: 0.699410990974, the score without that feature is 0.703466408039\n",
      "The score with feature BLS_Depth_1_0 is: 0.699410990974, the score without that feature is 0.721974298407\n",
      "The score is higher without feature BLS_Depth_1_0 so it has been dropped\n",
      "The score with feature BLS_Qtran_1_0 is: 0.721974298407, the score without that feature is 0.6843419566\n",
      "The score with feature BLS_Qingress_1_0 is: 0.721974298407, the score without that feature is 0.705737072347\n",
      "The score with feature BLS_OOTmag_1_0 is: 0.721974298407, the score without that feature is 0.710733512794\n",
      "The score with feature BLS_i1_1_0 is: 0.721974298407, the score without that feature is 0.688955495642\n",
      "The score with feature BLS_i2_1_0 is: 0.721974298407, the score without that feature is 0.699992015513\n",
      "The score with feature BLS_deltaChi2_1_0 is: 0.721974298407, the score without that feature is 0.667301434733\n",
      "The score with feature BLS_fraconenight_1_0 is: 0.721974298407, the score without that feature is 0.705737072347\n",
      "The score with feature BLS_Npointsintransit_1_0 is: 0.721974298407, the score without that feature is 0.710733512794\n",
      "The score with feature BLS_Ntransits_1_0 is: 0.721974298407, the score without that feature is 0.715729953241\n",
      "The score with feature BLS_Npointsbeforetransit_1_0 is: 0.721974298407, the score without that feature is 0.704434002432\n",
      "The score with feature BLS_Npointsaftertransit_1_0 is: 0.721974298407, the score without that feature is 0.715170838132\n",
      "The score with feature BLS_Rednoise_1_0 is: 0.721974298407, the score without that feature is 0.703905570515\n",
      "The score with feature BLS_Whitenoise_1_0 is: 0.721974298407, the score without that feature is 0.704861219984\n",
      "The score with feature BLS_SignaltoPinknoise_1_0 is: 0.721974298407, the score without that feature is 0.67636058782\n",
      "The score with feature P0 is: 0.721974298407, the score without that feature is 0.721974298407\n",
      "The score with feature P1 is: 0.721974298407, the score without that feature is 0.693938853933\n",
      "The score with feature P2 is: 0.721974298407, the score without that feature is 0.70257779817\n",
      "The score with feature P3 is: 0.721974298407, the score without that feature is 0.717003889458\n",
      "The score with feature P4 is: 0.721974298407, the score without that feature is 0.705737072347\n",
      "The score with feature P5 is: 0.721974298407, the score without that feature is 0.710733512794\n",
      "The score with feature P6 is: 0.721974298407, the score without that feature is 0.700298066908\n",
      "The score with feature P7 is: 0.721974298407, the score without that feature is 0.703154582109\n",
      "The score with feature P8 is: 0.721974298407, the score without that feature is 0.698104443179\n",
      "The score with feature P9 is: 0.721974298407, the score without that feature is 0.710213247724\n",
      "The score with feature P10 is: 0.721974298407, the score without that feature is 0.705255657316\n",
      "The score with feature P11 is: 0.721974298407, the score without that feature is 0.705255657316\n",
      "The score with feature P12 is: 0.721974298407, the score without that feature is 0.708888928806\n",
      "The score with feature P13 is: 0.721974298407, the score without that feature is 0.705255657316\n",
      "The score with feature P14 is: 0.721974298407, the score without that feature is 0.708888928806\n",
      "The score with feature P15 is: 0.721974298407, the score without that feature is 0.697568140699\n",
      "The score with feature P16 is: 0.721974298407, the score without that feature is 0.721424402787\n",
      "The score with feature P17 is: 0.721974298407, the score without that feature is 0.70758745564\n",
      "The score with feature P18 is: 0.721974298407, the score without that feature is 0.703466408039\n",
      "The score with feature P19 is: 0.721974298407, the score without that feature is 0.704861219984\n",
      "the dropped features are: ['BLS_Depth_1_0']\n"
     ]
    }
   ],
   "source": [
    "X = data_combined_features.drop(['Ids', 'CatalogY', 'ManuleY', 'CombinedY',\n",
    "                                 'Catalog_Period', 'Depth', 'Catalog_Epoch', 'SNR'],\n",
    "                                axis=1)\n",
    "\n",
    "def feature_testing(alg, X, y, threshold):\n",
    "    score_perm = modelfit(alg, X, y)  # test the model without the feature dropped\n",
    "    drop_list = []\n",
    "    print 'testing features linearly based on threshold'\n",
    "    X = X\n",
    "    X_copy = X.copy()\n",
    "    for column in X.columns:\n",
    "        df_temp = X.drop(column, axis=1)  # temporarily drop features\n",
    "        score_temp = modelfit(alg, df_temp, y)  # test the model with the feature dropped\n",
    "        print 'The score with feature {0} is: {1}, the score without that feature is {2}'.format(column, score_perm,\n",
    "                                                                                                 score_temp)\n",
    "        \n",
    "        if score_temp - score_perm > threshold: \n",
    "            X.drop(column, axis=1, inplace=True)\n",
    "            score_perm = modelfit(alg, X, y)\n",
    "            drop_list.append(column)\n",
    "            \n",
    "            print 'The score is higher without feature {0} so it has been dropped'.format(column)\n",
    "    print 'the dropped features are: {0}'.format(drop_list)\n",
    "\n",
    "feature_testing(xgb1, X, y, .005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, introducing a threshold reduced the amount of dropped features, but had a bigger impact on the score, increasing the max. score to 0.721974298407. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing features linearly in reverse\n",
      "The score with feature P19 is: 0.699410990974, the score without that feature is 0.70257779817\n",
      "The score with feature P18 is: 0.699410990974, the score without that feature is 0.696762713372\n",
      "The score with feature P17 is: 0.699410990974, the score without that feature is 0.688955495642\n",
      "The score with feature P16 is: 0.699410990974, the score without that feature is 0.699410990974\n",
      "The score with feature P15 is: 0.699410990974, the score without that feature is 0.698922212224\n",
      "The score with feature P14 is: 0.699410990974, the score without that feature is 0.697122253662\n",
      "The score with feature P13 is: 0.699410990974, the score without that feature is 0.698521507342\n",
      "The score with feature P12 is: 0.699410990974, the score without that feature is 0.703466408039\n",
      "The score with feature P11 is: 0.699410990974, the score without that feature is 0.702092662611\n",
      "The score with feature P10 is: 0.699410990974, the score without that feature is 0.694387979516\n",
      "The score with feature P9 is: 0.699410990974, the score without that feature is 0.7007406319\n",
      "The score with feature P8 is: 0.699410990974, the score without that feature is 0.6953404765\n",
      "The score with feature P7 is: 0.699410990974, the score without that feature is 0.708411308736\n",
      "The score is higher without feature P7 so it has been dropped\n",
      "The score with feature P6 is: 0.708411308736, the score without that feature is 0.708888928806\n",
      "The score with feature P5 is: 0.708411308736, the score without that feature is 0.705255657316\n",
      "The score with feature P4 is: 0.708411308736, the score without that feature is 0.697122253662\n",
      "The score with feature P3 is: 0.708411308736, the score without that feature is 0.705737072347\n",
      "The score with feature P2 is: 0.708411308736, the score without that feature is 0.705737072347\n",
      "The score with feature P1 is: 0.708411308736, the score without that feature is 0.703905570515\n",
      "The score with feature P0 is: 0.708411308736, the score without that feature is 0.708411308736\n",
      "The score with feature BLS_SignaltoPinknoise_1_0 is: 0.708411308736, the score without that feature is 0.672466013053\n",
      "The score with feature BLS_Whitenoise_1_0 is: 0.708411308736, the score without that feature is 0.70706307156\n",
      "The score with feature BLS_Rednoise_1_0 is: 0.708411308736, the score without that feature is 0.6953404765\n",
      "The score with feature BLS_Npointsaftertransit_1_0 is: 0.708411308736, the score without that feature is 0.70257779817\n",
      "The score with feature BLS_Npointsbeforetransit_1_0 is: 0.708411308736, the score without that feature is 0.717003889458\n",
      "The score is higher without feature BLS_Npointsbeforetransit_1_0 so it has been dropped\n",
      "The score with feature BLS_Ntransits_1_0 is: 0.717003889458, the score without that feature is 0.710733512794\n",
      "The score with feature BLS_Npointsintransit_1_0 is: 0.717003889458, the score without that feature is 0.70257779817\n",
      "The score with feature BLS_fraconenight_1_0 is: 0.717003889458, the score without that feature is 0.70257779817\n",
      "The score with feature BLS_deltaChi2_1_0 is: 0.717003889458, the score without that feature is 0.700669725477\n",
      "The score with feature BLS_i2_1_0 is: 0.717003889458, the score without that feature is 0.698104443179\n",
      "The score with feature BLS_i1_1_0 is: 0.717003889458, the score without that feature is 0.700298066908\n",
      "The score with feature BLS_OOTmag_1_0 is: 0.717003889458, the score without that feature is 0.712033480509\n",
      "The score with feature BLS_Qingress_1_0 is: 0.717003889458, the score without that feature is 0.703905570515\n",
      "The score with feature BLS_Qtran_1_0 is: 0.717003889458, the score without that feature is 0.690747751006\n",
      "The score with feature BLS_Depth_1_0 is: 0.717003889458, the score without that feature is 0.708204721039\n",
      "The score with feature BLS_SDE_1_0 is: 0.717003889458, the score without that feature is 0.710733512794\n",
      "The score with feature BLS_SR_1_0 is: 0.717003889458, the score without that feature is 0.70257779817\n",
      "The score with feature BLS_SN_1_0 is: 0.717003889458, the score without that feature is 0.687548825758\n",
      "The score with feature BLS_Tc_1_0 is: 0.717003889458, the score without that feature is 0.713872287098\n",
      "The score with feature BLS_Period_1_0 is: 0.717003889458, the score without that feature is 0.693054304249\n",
      "the dropped features are: ['P7', 'BLS_Npointsbeforetransit_1_0']\n"
     ]
    }
   ],
   "source": [
    "X = data_combined_features.drop(['Ids', 'CatalogY', 'ManuleY', 'CombinedY',\n",
    "                                 'Catalog_Period', 'Depth', 'Catalog_Epoch', 'SNR'],\n",
    "                                axis=1)\n",
    "\n",
    "def feature_testing(alg, X, y, threshold):\n",
    "    score_perm = modelfit(alg, X, y)  # test the model without the feature dropped\n",
    "    drop_list = []\n",
    "    print 'testing features linearly in reverse'\n",
    "    X = X\n",
    "    X_copy = X.copy()\n",
    "    for column in reversed(X.columns):\n",
    "        df_temp = X.drop(column, axis=1)  # temporarily drop features\n",
    "        score_temp = modelfit(alg, df_temp, y)  # test the model with the feature dropped\n",
    "        print 'The score with feature {0} is: {1}, the score without that feature is {2}'.format(column, score_perm,\n",
    "                                                                                                 score_temp)\n",
    "        \n",
    "        if score_temp - score_perm > threshold: \n",
    "            X.drop(column, axis=1, inplace=True)\n",
    "            score_perm = modelfit(alg, X, y)\n",
    "            drop_list.append(column)\n",
    "            \n",
    "            print 'The score is higher without feature {0} so it has been dropped'.format(column)\n",
    "    print 'the dropped features are: {0}'.format(drop_list)\n",
    "\n",
    "feature_testing(xgb1, X, y, .005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going in the opposite direction once again increases the score, but not to the extent it previously does. \n",
    "\n",
    "Lets see what happens if we drop one feature at a time, test its effect on the the score, and then return it, at the end, we'll remove the feature that causes the greatest score difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing features linearly from left to right\n",
      "The largest difference was: 0.0225633074325, it was caused by feature BLS_Depth_1_0\n"
     ]
    }
   ],
   "source": [
    "X = data_combined_features.drop(['Ids', 'CatalogY', 'ManuleY', 'CombinedY',\n",
    "                                 'Catalog_Period', 'Depth', 'Catalog_Epoch', 'SNR'],\n",
    "                                axis=1)\n",
    "\n",
    "def feature_testing(alg, X, y):\n",
    "    score_perm = modelfit(alg, X, y)  # test the model without the feature dropped\n",
    "    differences = []\n",
    "    X = X\n",
    "    X_copy = X.copy()\n",
    "    for column in X.columns:\n",
    "        df_temp = X.drop(column, axis=1)  # temporarily drop features\n",
    "        score_temp = modelfit(alg, df_temp, y)  # test the model with the feature dropped\n",
    "        differences.append(score_temp - score_perm) \n",
    "    print 'The largest difference was: {0}, it was caused by feature {1}'.format(max(differences),\n",
    "                                                                                 X.columns[differences.index(max(differences))])   \n",
    "\n",
    "feature_testing(xgb1, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Which is actually the difference we found by setting the threshold to .005 earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across the tests the best score we got was 0.721974298407 by deleting 'BLS_Depth_1_0'. \n",
    "\n",
    "We also observed that the max score we get is dependant both on the order we deleted the features and the method used to delete them. \n",
    "\n",
    "A next step could be to start deleting features in multiples; 2, 3, etc. at a time and observing the effect. Also, instead of deleting the features linearly from one side to the next we could delete them randomly. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
