{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the TSNE features on XGBoost \n",
    "\n",
    "The following TSNE reductions were generated by first using TruncatedSVD to get them to 50 features and then reduced down to several different lower dimensions using TSNE.  \n",
    "\n",
    "In the first cell first the LC reductions are concatenated to the combined feature file and tested on the XGBoost default parameters, in the second cell the PCA features are first removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the base score on the combined feature file is: 0.793499017458\n",
      "Conatenating 2D reduction of LC\n",
      "0.788332824602\n",
      "Conatenating 3D reduction of LC\n",
      "0.791029337864\n",
      "Conatenating 10D reduction of LC\n",
      "0.791350644396\n",
      "Conatenating 20D reduction of LC\n",
      "0.78543797189\n",
      "Concatenating 2D reduction of phase_space transformation\n",
      "0.793499017458\n",
      "Concatenating 3D reduction of phase_space transformation\n",
      "0.793499017458\n",
      "Concatenating 10D reduction of phase_space transformation\n",
      "0.793499017458\n",
      "Concatenating 20D reduction of phase_space transformation\n",
      "0.793499017458\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# these are the various reduction files \n",
    "\n",
    "# 10 dimension reduction\n",
    "tend_LC = pd.read_csv('tend_LC.csv', header=0, index_col =0)\n",
    "tend_phase = pd.read_csv('tend_phase.csv', header=0, index_col=0)\n",
    "\n",
    "# 3 dimension reduction\n",
    "threed_LC = pd.read_csv('threed_LC.csv', header=0, index_col=0)\n",
    "threed_phase = pd.read_csv('threed_phase.csv', header=0, index_col=0)\n",
    "\n",
    "# 20 dimension reduction\n",
    "twentyd_LC = pd.read_csv('twentyd_LC.csv', header=0, index_col=0)\n",
    "twentyd_phase = pd.read_csv('twentyd_phase.csv', header=0, index_col=0)\n",
    "\n",
    "# 2 dimension reduction \n",
    "twod_LC = pd.read_csv('twod_LC.csv', header=0, index_col=0)\n",
    "twod_phase = pd.read_csv('twod_phase.csv', header=0, index_col=0)\n",
    "\n",
    "# opening the combined feature file  \n",
    "\n",
    "data_combined_features = pd.read_csv(\"TESSfield_05h_01d_combinedfeatures.csv\",\n",
    "                                     header=0, index_col=0)\n",
    "data_combined_features = data_combined_features.drop(data_combined_features.index[-1])\n",
    "\n",
    "# drop the columns that aren't features and get targets \n",
    "X_2 = data_combined_features.drop(['Ids', 'CatalogY', 'ManuleY', 'CombinedY',\n",
    "                                 'Catalog_Period', 'Depth', 'Catalog_Epoch', \n",
    "                                 'SNR', 'BLS_Depth_1_0'],\n",
    "                                axis=1)\n",
    "\n",
    "# drop the columns that aren't features and get targets \n",
    "X = data_combined_features.drop(['Ids', 'CatalogY', 'ManuleY', 'CombinedY',\n",
    "                                 'Catalog_Period', 'Depth', 'Catalog_Epoch', \n",
    "                                 'SNR'],\n",
    "                                axis=1)\n",
    "y = data_combined_features['CombinedY']\n",
    "\n",
    "\n",
    "def modelfit(alg, X, y, cv_folds=4):\n",
    "    # StratifiedKFold automatically used by cross_val_predict on binary classification\n",
    "    # bear in mind that this does not use trapezfoid rule\n",
    "    # y_pred calculates the probabilities that each value is 1 or 0 using stratified cross validation\n",
    "    # pr_auc calculates the area under a precision recall curve\n",
    "    y_pred = cross_val_predict(alg, X, y, cv=cv_folds, \n",
    "                               method='predict_proba')[:, 1]\n",
    "    pr_auc = metrics.average_precision_score(y, y_pred)\n",
    "    return pr_auc\n",
    "\n",
    "xgb1 = XGBClassifier(objective='binary:logistic')\n",
    "\n",
    "print 'the base score on the combined feature file is: {0}'.format(modelfit(xgb1, X, y))\n",
    "\n",
    "# list of reduction references \n",
    "reductions_LC = [twod_LC, threed_LC, tend_LC, twentyd_LC]\n",
    "reductions_phase = [twod_phase, threed_phase, tend_phase, twentyd_phase]\n",
    "\n",
    "# concatenating the LC and Phase reductions to the combined feature file\n",
    "# testing on all the variations\n",
    "for reductions in reductions_LC:\n",
    "    combination = pd.concat([reductions, X], axis=1)\n",
    "    print 'Conatenating {0}D reduction of LC'.format(len(reductions.columns))\n",
    "    print modelfit(xgb1, combination, y)\n",
    "    \n",
    "for reductions in reductions_phase:\n",
    "    reductions = reductions.drop(reductions.index[-1]) # need to drop last to make files line up\n",
    "    print 'Concatenating {0}D reduction of phase_space transformation'.format(len(reductions.columns))\n",
    "    combination = pd.concat([reductions, X], axis=1)\n",
    "    print modelfit(xgb1, combination, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As we can see above, appending the various reductions to the combined feature files does nothing positive for the score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll see if substituting TSNE for the PCA features causes a better score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'BLS_Period_1_0', u'BLS_Tc_1_0', u'BLS_SN_1_0', u'BLS_SR_1_0',\n",
      "       u'BLS_SDE_1_0', u'BLS_Depth_1_0', u'BLS_Qtran_1_0', u'BLS_Qingress_1_0',\n",
      "       u'BLS_OOTmag_1_0', u'BLS_i1_1_0', u'BLS_i2_1_0', u'BLS_deltaChi2_1_0',\n",
      "       u'BLS_fraconenight_1_0', u'BLS_Npointsintransit_1_0',\n",
      "       u'BLS_Ntransits_1_0', u'BLS_Npointsbeforetransit_1_0',\n",
      "       u'BLS_Npointsaftertransit_1_0', u'BLS_Rednoise_1_0',\n",
      "       u'BLS_Whitenoise_1_0', u'BLS_SignaltoPinknoise_1_0'],\n",
      "      dtype='object')\n",
      "Conatenating 2D reduction of LC\n",
      "0.787309206428\n",
      "Conatenating 3D reduction of LC\n",
      "0.782768907446\n",
      "Conatenating 10D reduction of LC\n",
      "0.779340888349\n",
      "Conatenating 20D reduction of LC\n",
      "0.780738668802\n",
      "Concatenating 2D reduction of phase_space\n",
      "0.784428393876\n",
      "Concatenating 3D reduction of phase_space\n",
      "0.784428393876\n",
      "Concatenating 10D reduction of phase_space\n",
      "0.784428393876\n",
      "Concatenating 20D reduction of phase_space\n",
      "0.784428393876\n"
     ]
    }
   ],
   "source": [
    "# performing the same but dropping the PCA features\n",
    "\n",
    "X = X.drop(X.columns[20:], axis=1)\n",
    "\n",
    "print X.columns # making sure columns are dropped\n",
    "\n",
    "for reductions in reductions_LC:\n",
    "    combination = pd.concat([reductions, X], axis=1)\n",
    "    print 'Conatenating {0}D reduction of LC'.format(len(reductions.columns))\n",
    "    print modelfit(xgb1, combination, y)\n",
    "    \n",
    "for reductions in reductions_phase:\n",
    "    reductions = reductions.drop(reductions.index[-1]) # need to drop last to make files line up\n",
    "    print 'Concatenating {0}D reduction of phase_space'.format(len(reductions.columns))\n",
    "    combination = pd.concat([reductions, X], axis=1)\n",
    "    print modelfit(xgb1, combination, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the PCA features perform better than the TSNE/TSVD reductions in this configuration. \n",
    "\n",
    "Of course there is an extreme amount of variation that can be created by parameter tuning TSNE/TSVD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- TSNE: https://lvdmaaten.github.io/tsne/\n",
    "- TSVD: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "- XGBoost: https://xgboost.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
