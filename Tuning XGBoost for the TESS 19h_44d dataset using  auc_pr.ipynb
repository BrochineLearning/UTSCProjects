{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the XGBoost classification algorithm on TESS data using the pr_auc metric \n",
    "\n",
    "The following is a tuning of the XGBoost algorithm to the pr_auc error metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "df = pd.read_csv(\"TESSfield_19h_44d_combinedfeatures_try2.csv\", index_col=0)\n",
    "X = df.drop(['Ids', 'CatalogY', 'ManuleY', 'CombinedY', 'Catalog_Period',\n",
    "             'Depth', 'Catalog_Epoch', 'SNR'], axis=1)\n",
    "\n",
    "y = df['CombinedY']\n",
    "\n",
    "def modelfit(alg, X, y, cv_folds=4):\n",
    "    # StratifiedKFold automatically used by cross_val_predict on binary classification\n",
    "    # bear in mind that this does not use trapezfoid rule\n",
    "    # y_pred calculates the probabilities that each value is 1 or 0 using stratified cross validation\n",
    "    # pr_auc calculates the area under a precision recall curve\n",
    "    y_pred = cross_val_predict(alg, X, y, method='predict_proba', cv=cv_folds)[:,1]\n",
    "\n",
    "    \n",
    "    pr_auc = metrics.average_precision_score(y, y_pred)\n",
    "    \n",
    "    return pr_auc\n",
    "\n",
    "xgb1 = XGBClassifier(objective='binary:logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will get a baseline score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75080326356931726"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelfit(xgb1, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try adjusting depth, we will try 1, 6, 12, and 18. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing depth value 1\n",
      "0.764009207239\n",
      "testing depth value 6\n",
      "0.731479877352\n",
      "testing depth value 12\n",
      "0.756981717936\n",
      "testing depth value 18\n",
      "0.75714369004\n",
      "testing depth value 40\n",
      "0.750377616871\n"
     ]
    }
   ],
   "source": [
    "depth_vals = [1, 6, 12, 18, 40]\n",
    "\n",
    "for vals in depth_vals:\n",
    "    print 'testing depth value {0}'.format(vals)\n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=vals,    \n",
    "        objective='binary:logistic')\n",
    "    \n",
    "    print modelfit(xgb, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score is at 1. Next is colsample_bytree, with standard values between .5-1; we'll try lower values as well to be safe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing colsample_bytree value 0.2\n",
      "0.760282454355\n",
      "testing colsample_bytree value 0.4\n",
      "0.764976955446\n",
      "testing colsample_bytree value 0.6\n",
      "0.764551806212\n",
      "testing colsample_bytree value 0.8\n",
      "0.7636037221\n",
      "testing colsample_bytree value 1\n",
      "0.764009207239\n"
     ]
    }
   ],
   "source": [
    "colsample_vals = [.2, .4 ,.6,.8, 1]\n",
    "\n",
    "for vals in colsample_vals:\n",
    "    print 'testing colsample_bytree value {0}'.format(vals)\n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=1,\n",
    "        colsample_bytree=vals,\n",
    "        objective='binary:logistic')\n",
    "    print modelfit(xgb, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ .7649 is the best with a colsample_bytree value of .4.\n",
    "\n",
    "Next is subsample, we'll try the same values as above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing subsample value 0.2\n",
      "0.767834899619\n",
      "testing subsample value 0.4\n",
      "0.768505268033\n",
      "testing subsample value 0.6\n",
      "0.767990545297\n",
      "testing subsample value 0.8\n",
      "0.768153950397\n",
      "testing subsample value 1\n",
      "0.764976955446\n"
     ]
    }
   ],
   "source": [
    "subsample_vals = [.2, .4, .6, .8, 1]\n",
    "\n",
    "for vals in subsample_vals:\n",
    "    print 'testing subsample value {0}'.format(vals)\n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=1,\n",
    "        colsample_bytree=.4,\n",
    "        subsample=vals,\n",
    "        objective='binary:logistic')\n",
    "    print modelfit(xgb, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best score is ~.7685 with a subsample val of .4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is min child weight typically in the range of 0.1 \n",
    "to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing min_child value 0.1\n",
      "0.768170594879\n",
      "testing min_child value 0.5\n",
      "0.768403483305\n",
      "testing min_child value 1\n",
      "0.768505268033\n",
      "testing min_child value 3\n",
      "0.768557212882\n",
      "testing min_child value 5\n",
      "0.768665280547\n",
      "testing min_child value 7\n",
      "0.768988990562\n",
      "testing min_child value 10\n",
      "0.768939362254\n",
      "testing min_child value 175\n",
      "0.622267488795\n"
     ]
    }
   ],
   "source": [
    "min_child_weight = [.1, .5, 1, 3, 5, 7, 10, 175]\n",
    "\n",
    "for vals in min_child_weight:\n",
    "    print 'testing min_child value {0}'.format(vals)\n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=1,\n",
    "        colsample_bytree=.4,\n",
    "        subsample=.4,\n",
    "        min_child_weight=vals,\n",
    "        objective='binary:logistic')\n",
    "    print modelfit(xgb, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here 7 gives us the max score of ~0.7689, finally we'll check learning rate against various number of estimators as they interact heavily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing rates value 0.001, with n_estimators 100\n",
      "0.635061402304\n",
      "testing rates value 0.001, with n_estimators 500\n",
      "0.648475374884\n",
      "testing rates value 0.001, with n_estimators 1000\n",
      "0.65049502959\n",
      "testing rates value 0.001, with n_estimators 3000\n",
      "0.666099042473\n",
      "testing rates value 0.001, with n_estimators 5000\n",
      "0.735681842486\n",
      "testing rates value 0.001, with n_estimators 7000\n",
      "0.760974993808\n",
      "testing rates value 0.001, with n_estimators 9000\n",
      "0.76735242654\n",
      "testing rates value 0.001, with n_estimators 11000\n",
      "0.770437025791\n",
      "testing rates value 0.001, with n_estimators 13000\n",
      "0.772355568412\n",
      "testing rates value 0.01, with n_estimators 100\n",
      "0.648819177507\n",
      "testing rates value 0.01, with n_estimators 500\n",
      "0.73497494105\n",
      "testing rates value 0.01, with n_estimators 1000\n",
      "0.768712776119\n",
      "testing rates value 0.01, with n_estimators 3000\n",
      "0.776149067003\n",
      "testing rates value 0.01, with n_estimators 5000\n",
      "0.774461009558\n",
      "testing rates value 0.01, with n_estimators 7000\n",
      "0.771426764645\n",
      "testing rates value 0.01, with n_estimators 9000\n",
      "0.76867145135\n",
      "testing rates value 0.01, with n_estimators 11000\n",
      "0.765797137012\n",
      "testing rates value 0.01, with n_estimators 13000\n",
      "0.762835844168\n",
      "testing rates value 0.05, with n_estimators 100\n",
      "0.730819661839\n",
      "testing rates value 0.05, with n_estimators 500\n",
      "0.775687915152\n",
      "testing rates value 0.05, with n_estimators 1000\n",
      "0.774308949054\n",
      "testing rates value 0.05, with n_estimators 3000\n",
      "0.759376852303\n",
      "testing rates value 0.05, with n_estimators 5000\n",
      "0.749147778965\n",
      "testing rates value 0.05, with n_estimators 7000\n",
      "0.741290675253\n",
      "testing rates value 0.05, with n_estimators 9000\n",
      "0.735823045654\n",
      "testing rates value 0.05, with n_estimators 11000\n",
      "0.728439047902\n",
      "testing rates value 0.05, with n_estimators 13000\n",
      "0.723710731988\n",
      "testing rates value 0.07, with n_estimators 100\n",
      "0.758953178232\n",
      "testing rates value 0.07, with n_estimators 500\n",
      "0.775715395904\n",
      "testing rates value 0.07, with n_estimators 1000\n",
      "0.77174493118\n",
      "testing rates value 0.07, with n_estimators 3000\n",
      "0.752150559123\n",
      "testing rates value 0.07, with n_estimators 5000\n",
      "0.740443132704\n",
      "testing rates value 0.07, with n_estimators 7000\n",
      "0.733110630644\n",
      "testing rates value 0.07, with n_estimators 9000\n",
      "0.725428548286\n",
      "testing rates value 0.07, with n_estimators 11000\n",
      "0.716660357069\n",
      "testing rates value 0.07, with n_estimators 13000\n",
      "0.711446754362\n",
      "testing rates value 0.09, with n_estimators 100\n",
      "0.766741649361\n",
      "testing rates value 0.09, with n_estimators 500\n",
      "0.775557170972\n",
      "testing rates value 0.09, with n_estimators 1000\n",
      "0.769895976222\n",
      "testing rates value 0.09, with n_estimators 3000\n",
      "0.747074560189\n",
      "testing rates value 0.09, with n_estimators 5000\n",
      "0.734782428937\n",
      "testing rates value 0.09, with n_estimators 7000\n",
      "0.72731607264\n",
      "testing rates value 0.09, with n_estimators 9000\n",
      "0.718605291059\n",
      "testing rates value 0.09, with n_estimators 11000\n",
      "0.707539070162\n",
      "testing rates value 0.09, with n_estimators 13000\n",
      "0.70136888141\n",
      "testing rates value 0.1, with n_estimators 100\n",
      "0.768988990562\n",
      "testing rates value 0.1, with n_estimators 500\n",
      "0.774952763343\n",
      "testing rates value 0.1, with n_estimators 1000\n",
      "0.769467406853\n",
      "testing rates value 0.1, with n_estimators 3000\n",
      "0.74480086107\n",
      "testing rates value 0.1, with n_estimators 5000\n",
      "0.731379236563\n",
      "testing rates value 0.1, with n_estimators 7000\n",
      "0.722207045179\n",
      "testing rates value 0.1, with n_estimators 9000\n",
      "0.71383098719\n",
      "testing rates value 0.1, with n_estimators 11000\n",
      "0.703764081387\n",
      "testing rates value 0.1, with n_estimators 13000\n",
      "0.697975816737\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [1e-3, 1e-2, .05, .07, \n",
    "                .09, .1]\n",
    "n_estimators = [100, 500, 1000, 3000, 5000,\n",
    "               7000, 9000, 11000, 13000]\n",
    "\n",
    "for rates in learning_rate:\n",
    "    for estimators in n_estimators:\n",
    "        print 'testing rates value {0}, with n_estimators {1}'.format(rates, \n",
    "                                                                      estimators)\n",
    "        xgb = XGBClassifier(\n",
    "            max_depth=1,\n",
    "            colsample_bytree=.4,\n",
    "            subsample= .4,\n",
    "            min_child_weight=7,\n",
    "            n_estimators=estimators, \n",
    "            learning_rate=rates,\n",
    "            objective='binary:logistic')\n",
    "            \n",
    "        print modelfit(xgb, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the best eta value is 0.01 with n_estimators 3000\n",
    "and a score of 0.776149067003. Next we'll get the final baseline pr_auc and roc_auc then see how it performs on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pr_auc score is: 0.776149067003\n",
      "The roc_auc score is: 0.909755763766\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "          max_depth=1,\n",
    "          colsample_bytree=.4,\n",
    "          subsample= .4,\n",
    "          min_child_weight=7,\n",
    "          n_estimators=3000, \n",
    "          learning_rate=.01,\n",
    "          objective='binary:logistic')\n",
    "\n",
    "y_pred = cross_val_predict(xgb, X, y, cv=4, method='predict_proba')[:, 1]\n",
    "pr_auc = metrics.average_precision_score(y, y_pred)\n",
    "roc_auc = metrics.roc_auc_score(y, y_pred)\n",
    "\n",
    "print 'The pr_auc score is: {0}'.format(pr_auc)\n",
    "print 'The roc_auc score is: {0}'.format(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the model on the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"TESSfield_12h_-20d_combinedfeatures_try2.csv\", index_col=0)\n",
    "X_test = df_test.drop(['Ids', 'CatalogY', 'ManuleY', 'CombinedY', 'Catalog_Period',\n",
    "             'Depth', 'Catalog_Epoch', 'SNR'], axis=1)\n",
    "\n",
    "y_test = df_test['CombinedY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pr_auc score is: 0.739524668741\n",
      "The roc_auc score is: 0.906274573981\n"
     ]
    }
   ],
   "source": [
    "xgb.fit(X, y) # fitting on the 19h set\n",
    "\n",
    "y_pred_test = xgb.predict_proba(X_test)[:, 1] # testing on the 12h set \n",
    "\n",
    "pr_auc = metrics.average_precision_score(y_test, y_pred_test)\n",
    "roc_auc = metrics.roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print 'The pr_auc score is: {0}'.format(pr_auc)\n",
    "print 'The roc_auc score is: {0}'.format(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
